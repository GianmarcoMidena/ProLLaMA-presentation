\begin{frame}{Experiment Setup}
	\begin{center}
		\begin{tabular}{|>{\centering\arraybackslash}p{11em}|>{\centering\arraybackslash}p{6em}|>{\centering\arraybackslash}p{6em}|}\hline
			Learning Stage & Continual Learning & Instruction Tuning\\\hline
			LoRA rank & 128 & 64 \\\hline
			epochs    & 1   & 2 \\\hline
			max sequence length (block size) & 2048 & 256 \\\hline
			batch size per GPU & 4 & 144 \\\hline
			gradient accumulation step count & 8 & 4 \\\hline
			scheduler warm-up ratio & 0.05 & 0.03 \\\hline
			optimizer & \multicolumn{2}{c|}{AdamW} \\\hline
			weight decay & \multicolumn{2}{c|}{0.01} \\\hline
			scheduler & \multicolumn{2}{c|}{cosine annealing with warm-up} \\\hline
			Zero Redundancy Optimizer (ZeRO) & \multicolumn{2}{c|}{stage 2, w/o offload} \\\hline
			data type & \multicolumn{2}{c|}{bfloat16} \\\hline
			%peak learning rate & \multicolumn{2}{c|}{0.05} \\\hline
		\end{tabular}
	\end{center}
\end{frame}
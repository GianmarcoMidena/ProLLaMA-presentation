\documentclass[dvipsnames,
%xcolor={svgnames},
hyperref={
	citecolor=blue,
	colorlinks=true,
	urlcolor=blue,
	linkcolor=,
}
]{beamer}
\beamertemplatenavigationsymbolsempty
\usetheme{Boadilla}
\usefonttheme[onlymath]{serif}

\usepackage{cleveref}

\usepackage{amsmath}
\usepackage{bm}
\usepackage{bbm}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage[cal=boondoxo]{mathalpha}

% Change horizontal spacing
\setlength{\tabcolsep}{3pt}

\usepackage[none]{hyphenat} % no hyphenation

\usepackage{array}

\usepackage{cancel}

\usepackage[style=authoryear,maxcitenames=2,backend=biber,citetracker=true]{biblatex}
\addbibresource{references.bib}

\usepackage{verbatim}

\usepackage{bigints}

\usepackage{makecell}

\DeclareCiteCommand{\citeauthor}
{\boolfalse{citetracker}%
	\boolfalse{pagetracker}%
	\usebibmacro{prenote}}
{\ifciteindex
	{\indexnames{labelname}}
	{}%
	\printtext[bibhyperref]{\printnames{labelname}}}
{\multicitedelim}
{\usebibmacro{postnote}}

\DeclareCiteCommand{\citeyear}
{\usebibmacro{prenote}}
{\bibhyperref{\printfield{year}}\bibhyperref{\printfield{extrayear}}}
{\multicitedelim}
{\usebibmacro{postnote}}

\newcommand{\credit}[2]{{\par\hfill \tiny #1 credit:~\itshape{\color{blue} \citeauthor{#2} (\citeyear{#2})}}}
\newcommand{\crediturl}[2]{{\par\hfill \tiny #1 credit:~\itshape{\color{blue} \url{#2}}}}
\let\oldcite\cite
\renewcommand{\cite}[1]{{\color{blue} \oldcite{#1}}}
\newcommand{\citefoot}[1]{{\color{blue} \citeauthor{#1} (\citeyear{#1})}}
\newcommand{\matr}[1]{#1}

\newcommand{\red}[1]{{\color{red} #1}}

\title[ProLLaMA]
{\href{https://doi.org/10.48550/arXiv.2402.16445}{ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing}}
%\subtitle{}
\author[Liuzhenghao Lv et al.]{Lv, Liuzhenghao, Zongying Lin, Hao Li, Yuyang Liu, Jiaxi Cui, Calvin Yu-Chian Chen, Li Yuan, and Yonghong Tian}
%\institute{Aalto University}
\date{}%3 July 2024}

\addtobeamertemplate{title page}{}{
\begin{center}
Submitted on 26 February 2024
\\\vspace{4em}Presenter: Gianmarco Midena
\\\vspace{1em}3 July 2024
\end{center}}

\begin{document}
\begin{frame}
\titlepage
\end{frame}

%\begin{frame}{Outline}
%\tableofcontents
%\end{frame}

\input{sections/multi_task_llms_nl_vs_pl.tex}

\begin{frame}{ProLLMs: Protein LLMs}
	\begin{itemize}\setlength\itemsep{1.5em}
		\item Protein sequences as the protein language
		\item PLP:Protein Language Processing~(\cite{bepler2021learning,ofer2021language})
		\item LLMs for protein design~(\cite{strokach2022deep,ferruz2022controllable})~
		\item Trained on vast protein corpus
		\item Pros: rapid generation of structurally plausible protein sequences
		\item Immense potential for biomedical and biotechnological innovations
		\item Challenge: extend capabilities beyond sequence generation
	\end{itemize}
\end{frame}

\input{sections/protein_language_models.tex}

\begin{frame}{Challenges}%Problem}
	\begin{itemize}\setlength\itemsep{1.5em}
		\item De novo design of long and structurally plausible protein
		sequences~(\cite{ferruz2022protgpt2}) %is highly challenging.
		\item Extend LLM capabilities beyond sequence generation
		\item Beyond protein language
		\begin{itemize}
			\item beyond protein sequences and co-evolutionary
			information
			\item need of protein function and property information
			\item protein language is not sufficient for some PLP tasks
			\begin{itemize}
				\item tasks: controllable protein generation, protein property prediction, \dots
				\item components: instruction (input), output
			\end{itemize}
			\item natural language ability~(\cite{xu2023protst,wang2023instructprotein})
		\end{itemize}
		\item Instruction following
		\item Training resource consumption
%		\begin{itemize}
%			\item Feasibility
%		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Limitations in Current LLMs for Protein Language}
	\begin{itemize}\setlength\itemsep{3em}
		\item Single-task
		\item Lack Natural Language Capabilities
		\begin{itemize}
			\item Protein Language cannot fully represent all components of some tasks (User instruction, expected output)
		\end{itemize}
		\item Insufficient Instruction Understanding
		\item High Training Resource Demands
	\end{itemize}
\end{frame}

\end{frame}

\end{frame}

\input{sections/model.tex}

\input{sections/learning_stages.tex}

\input{sections/evaluation_metrics.tex}

\input{sections/protein_structure_levels.tex}

\input{sections/data.tex}

\begin{frame}{Performance in (Unconditional) Protein Generation}
	\begin{center}
		\includegraphics[scale=0.21]{tables/methods_comparison.png}
		\begin{tabular}{>{\centering\arraybackslash}p{11.7em}|>{\centering\arraybackslash}p{3.1em}>{\centering\arraybackslash}p{2.7em}|>{\centering\arraybackslash}p{2.3em}>{\centering\arraybackslash}p{2.3em}|>{\centering\arraybackslash}p{2.3em}>{\centering\arraybackslash}p{2.3em}}
		{\scriptsize \makecell{Natural protein \\ \cite{alamdari2023protein}}} & \scalebox{.55}{\underline{68.25$\pm$17.85}} & \scalebox{.55}{3.09$\pm$0.63} & & & & \\\hline
		\end{tabular}
		\credit{(Modified) table}{lv2024prollama}
	\end{center}
	%\credit{Table}{lv2024prollama}
	\begin{itemize}
		\item ProLLaMA can generate proteins
		\begin{itemize}
			\item Structurally plausible
			\item Comparable to natural proteins
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Quality of Generated Protein w.r.t. Length}%ProLLaMA vs. ESM2 (Baseline) Model}
%\begin{center}
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\includegraphics[scale=0.23]{images/combined_length_plddt_zhexiantu.pdf}
			\includegraphics[scale=0.23]{images/combined_length_alntmscore_zhexiantu.pdf}
		\end{column}
		\begin{column}{0.5\textwidth}
			\includegraphics[scale=0.23]{images/combined_length_scperp_zhexiantu.pdf}
			\begin{itemize}
				\item ProLLaMA is able to capture long-range dependencies between amino acids
			\end{itemize}
		\end{column}
	\end{columns}
%\end{center}
\vspace{-2em}\credit{Image}{lv2024prollama}
\end{frame}

\begin{frame}{Performance in Controllable Protein Generation}% on Four Different Instructions
	\begin{center}
		\includegraphics[scale=0.21]{tables/controlled_generation_comparison.png}
	\end{center}
	\vspace{-1em}\credit{Table}{lv2024prollama}
	\begin{itemize}
		\item Given one instruction, \\ProLLaMA generates proteins with the desired functionalities
		\item High metrics mean proteins meet instructions
		\item Other models: uncontrollable generation
		\item Instructions: four superfamily descriptions
		%What do superfamilies are?
		\begin{itemize}
			\item SAM-MT: S-adenosyl-L-methionine-dependent methyltransferase
			\item TPHD: Tetratricopeptide-like helical domain
			\item Trx: Thioredoxin-like 
			\item CheY: CheY-like
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Generated vs. Natural Protein}
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\includegraphics[scale=0.7]{images/d.png}
			\includegraphics[scale=0.7]{images/f.png}
		\end{column}
		\begin{column}{0.5\textwidth}
			\includegraphics[scale=0.7]{images/e.png}
			\begin{itemize}
				\item Proteins generated by ProLLaMA are comparable to their
				natural counterparts in the same superfamily
			\end{itemize}
		\end{column}
	\end{columns}
	\credit{Image}{lv2024prollama}
\end{frame}

\begin{frame}{Visualization of Controllable Generated vs. Natural Proteins}
	\begin{center}
		\includegraphics[trim={0 0 90em 0},clip,scale=0.4]{images/protein_visualization.pdf}
		\includegraphics[trim={31.5em 0 57.4em 0},clip,scale=0.4]{images/protein_visualization.pdf}
		\includegraphics[trim={64em 0 30em 0},clip,scale=0.4]{images/protein_visualization.pdf}
		\includegraphics[trim={92em 0 0 0},clip,scale=0.4]{images/protein_visualization.pdf}
	\end{center}
	\vspace{-1em}\credit{Image}{lv2024prollama}
	\begin{itemize}
		\item Blue: generated proteins, yellow: natural proteins
		\item Similar in structure (function), different in sequence (novel)
	\end{itemize}
\end{frame}

\begin{frame}{Performance in Protein Property Prediction}
	\begin{center}
		\begin{tabular}{cc}\hline
			Property & Accuracy (\%) \\\hline
			OBFD     & 100 \\
			UPF0145  & 100 \\
			NACD     & 100 \\
			U3S      & 100 \\
			CCHC     & 95.24  \\
			Kazal    & 100 \\
			SAM-MT   & 93.67  \\
			TPHD     & 90.84  \\
			Trx      & 94.17  \\
			CheY     & 100 \\\hline
		\end{tabular}
	\end{center}
	\credit{Table}{lv2024prollama}
	\vspace{-1em}
	\begin{itemize}
		\item 10k test examples
		\item Accuracy
		\begin{equation}
			\frac{\sum_{i=1}^N|Y_i \cap \hat{Y_i}|}{\sum_{i=1}^N|\hat{Y_i}|}
		\end{equation}
		\begin{itemize}
			\item $Y_i$, $\hat{Y_i}$: true and predicted property (superfamily) sets
		\end{itemize}
	\end{itemize}
\end{frame}

\input{sections/natural_language_ability.tex}

\input{sections/summary.tex}

\section{References}
\begin{frame}[allowframebreaks]
\frametitle{References}
\printbibliography
\end{frame}

\begin{frame}{Experiment Setup}
	\begin{center}
		\begin{tabular}{|>{\centering\arraybackslash}p{11em}|>{\centering\arraybackslash}p{6em}|>{\centering\arraybackslash}p{6em}|}\hline
			Learning Stage & Continual Learning & Instruction Tuning\\\hline
			LoRA rank & 128 & 64 \\\hline
			epochs    & 1   & 2 \\\hline
			max sequence length (block size) & 2048 & 256 \\\hline
			batch size per GPU & 4 & 144 \\\hline
			gradient accumulation step count & 8 & 4 \\\hline
			scheduler warm-up ratio & 0.05 & 0.03 \\\hline
			optimizer & \multicolumn{2}{c|}{AdamW} \\\hline
			weight decay & \multicolumn{2}{c|}{0.01} \\\hline
			scheduler & \multicolumn{2}{c|}{cosine annealing with warm-up} \\\hline
			Zero Redundancy Optimizer (ZeRO) & \multicolumn{2}{c|}{stage 2, w/o offload} \\\hline
			data type & \multicolumn{2}{c|}{bfloat16} \\\hline
			%peak learning rate & \multicolumn{2}{c|}{0.05} \\\hline
		\end{tabular}
	\end{center}
\end{frame}

\begin{frame}{Sequences Generated w/o Instructions - Distributions}
	\begin{center}
		\includegraphics[scale=0.35]{images/plddt_scperp.pdf}
		\includegraphics[scale=0.35]{images/tm_sid.pdf}
	\end{center}
	\credit{Image}{lv2024prollama}
	\begin{itemize}
		\item 1 spot = 1 sequence
	\end{itemize}
\end{frame}

\end{document}
\documentclass[dvipsnames,
%xcolor={svgnames},
hyperref={
	citecolor=blue,
	colorlinks=true,
	urlcolor=blue,
	linkcolor=,
}
]{beamer}
\beamertemplatenavigationsymbolsempty
\usetheme{Boadilla}
\usefonttheme[onlymath]{serif}

\usepackage{cleveref}

\usepackage{amsmath}
\usepackage{bm}
\usepackage{bbm}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage[cal=boondoxo]{mathalpha}

% Change horizontal spacing
\setlength{\tabcolsep}{3pt}

\usepackage[none]{hyphenat} % no hyphenation

\usepackage{array}

\usepackage{cancel}

\usepackage[style=authoryear,maxcitenames=2,backend=biber,citetracker=true]{biblatex}
\addbibresource{references.bib}

\usepackage{verbatim}

\usepackage{bigints}

\usepackage{makecell}

\DeclareCiteCommand{\citeauthor}
{\boolfalse{citetracker}%
	\boolfalse{pagetracker}%
	\usebibmacro{prenote}}
{\ifciteindex
	{\indexnames{labelname}}
	{}%
	\printtext[bibhyperref]{\printnames{labelname}}}
{\multicitedelim}
{\usebibmacro{postnote}}

\DeclareCiteCommand{\citeyear}
{\usebibmacro{prenote}}
{\bibhyperref{\printfield{year}}\bibhyperref{\printfield{extrayear}}}
{\multicitedelim}
{\usebibmacro{postnote}}

\newcommand{\credit}[2]{{\par\hfill \tiny #1 credit:~\itshape{\color{blue} \citeauthor{#2} (\citeyear{#2})}}}
\newcommand{\crediturl}[2]{{\par\hfill \tiny #1 credit:~\itshape{\color{blue} \url{#2}}}}
\let\oldcite\cite
\renewcommand{\cite}[1]{{\color{blue} \oldcite{#1}}}
\newcommand{\citefoot}[1]{{\color{blue} \citeauthor{#1} (\citeyear{#1})}}
\newcommand{\matr}[1]{#1}

\newcommand{\red}[1]{{\color{red} #1}}

\title[ProLLaMA]
{\href{https://doi.org/10.48550/arXiv.2402.16445}{ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing}}
%\subtitle{}
\author[Liuzhenghao Lv et al.]{Lv, Liuzhenghao, Zongying Lin, Hao Li, Yuyang Liu, Jiaxi Cui, Calvin Yu-Chian Chen, Li Yuan, and Yonghong Tian}
%\institute{Aalto University}
\date{}%3 July 2024}

\addtobeamertemplate{title page}{}{
\begin{center}
Submitted on 26 February 2024
\\\vspace{4em}Presenter: Gianmarco Midena
\\\vspace{1em}3 July 2024
\end{center}}

\begin{document}
\begin{frame}
\titlepage
\end{frame}

%\begin{frame}{Outline}
%\tableofcontents
%\end{frame}

\input{sections/multi_task_llms_nl_vs_pl.tex}

\begin{frame}{ProLLMs: Protein LLMs}
	\begin{itemize}\setlength\itemsep{1.5em}
		\item Protein sequences as the protein language
		\item PLP:Protein Language Processing~(\cite{bepler2021learning,ofer2021language})
		\item LLMs for protein design~(\cite{strokach2022deep,ferruz2022controllable})~
		\item Trained on vast protein corpus
		\item Pros: rapid generation of structurally plausible protein sequences
		\item Immense potential for biomedical and biotechnological innovations
		\item Challenge: extend capabilities beyond sequence generation
	\end{itemize}
\end{frame}

\input{sections/protein_language_models.tex}

\begin{frame}{Challenges}%Problem}
	\begin{itemize}\setlength\itemsep{1.5em}
		\item De novo design of long and structurally plausible protein
		sequences~(\cite{ferruz2022protgpt2}) %is highly challenging.
		\item Extend LLM capabilities beyond sequence generation
		\item Beyond protein language
		\begin{itemize}
			\item beyond protein sequences and co-evolutionary
			information
			\item need of protein function and property information
			\item protein language is not sufficient for some PLP tasks
			\begin{itemize}
				\item tasks: controllable protein generation, protein property prediction, \dots
				\item components: instruction (input), output
			\end{itemize}
			\item natural language ability~(\cite{xu2023protst,wang2023instructprotein})
		\end{itemize}
		\item Instruction following
		\item Training resource consumption
%		\begin{itemize}
%			\item Feasibility
%		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Limitations in Current LLMs for Protein Language}
	\begin{itemize}\setlength\itemsep{3em}
		\item Single-task
		\item Lack Natural Language Capabilities
		\begin{itemize}
			\item Protein Language cannot fully represent all components of some tasks (User instruction, expected output)
		\end{itemize}
		\item Insufficient Instruction Understanding
		\item High Training Resource Demands
	\end{itemize}
\end{frame}

\begin{frame}{Goals}
\begin{itemize}\setlength\itemsep{3em}
	\item Protein engineering
	\item Protein fitness landscape understanding
	\item \cite{pan2021recent,ren2022proximal,song2023importance}
\end{itemize}
\end{frame}

\begin{frame}{ProLLaMA}
	\begin{itemize}
		\item Protein Language Processing
		\item Training Framework
		\begin{itemize}
			\item Any general LLM $\rightarrow$ ProLLM
			\item Two or More Stages
			\item Universal
			\item Efficient, low Overhead
			\item Scalable
		\end{itemize}
		\item Multi-tasking
		\begin{itemize}
			\item Unconditional Protein Sequence Generation
			\item Controllable Protein Sequence Generation
			\item Protein Property Prediction
			\item \dots
		\end{itemize}
		\item Language: Protein + Natural
		\item %Efficient Training: 
		LoRA: Low-Rank Adaptation~(\cite{hu2021lora})
		\begin{itemize}
			\item prevents catastrophic forgetting of natural language knowledge
			\item more scalability
			\item less training cost
		\end{itemize}
	\end{itemize}
\end{frame}

\input{sections/model.tex}

\input{sections/learning_stages.tex}

\input{sections/evaluation_metrics.tex}

\input{sections/protein_structure_levels.tex}

\input{sections/data.tex}

\begin{frame}{Preprocessing}
	\begin{itemize}
		\item adds specific prefixes and suffixes to each protein sequence
		\begin{itemize}\setlength\itemsep{1em}
			\item standardizes format
			\item reduces confusion
			\item aids (e.g., LLaMA2) in distinguishing the new protein language from its existing natural language knowledge
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Compared Models}
	\begin{itemize}
		\item CNN
		\begin{itemize}
			\item CARP~(\cite{alamdari2023protein})
			\begin{itemize}
				\item Task: Unconditional Protein Generation
			\end{itemize}
			\item LRAR~(\cite{alamdari2023protein})
			\begin{itemize}
				\item Task: Unconditional Protein Generation
			\end{itemize}
		\end{itemize}
		\item AutoEncoder
		\begin{itemize}
			\item ESM-1b~(\cite{rives2021biological})
			\begin{itemize}
				\item Tasks: Unconditional and Conditional Protein Generation
			\end{itemize}
			\item ESM-2~(\cite{lin2023evolutionary})
			\begin{itemize}
				\item Tasks: Unconditional and Conditional Protein Generation
			\end{itemize}
		\end{itemize}
		\item Diffusion
		\begin{itemize}
			\item EvoDiff~(\cite{alamdari2023protein})
			\begin{itemize}
				\item Tasks: Unconditional and Conditional Protein Generation
			\end{itemize}
		\end{itemize}
		\item LLM
		\begin{itemize}
			\item ProGPT2~(\cite{ferruz2022protgpt2})
			\begin{itemize}
				\item Tasks: Unconditional and Conditional Protein Generation
			\end{itemize}
			\item ProGen2~(\cite{nijkamp2023progen2})
			\begin{itemize}
				\item Tasks: Unconditional and Conditional Protein Generation
			\end{itemize}
		\end{itemize}
	\end{itemize}
\end{frame}

\input{sections/performance_unconditional_protein_generation.tex}

\input{sections/quality_unconditionally_generated_protein_vs_lenght.tex}

\input{sections/performance_controllable_protein_generation.tex}

\input{sections/quality_controllable_generated_vs_natural_protein.tex}

\input{sections/visualization_controllable_generated_vs_natural_proteins.tex}

\input{sections/performance_protein_property_prediction.tex}

\input{sections/natural_language_ability.tex}

\input{sections/summary.tex}

\input{sections/open_questions.tex}

\section{References}
\begin{frame}[allowframebreaks]
\frametitle{References}
\printbibliography
\end{frame}

\begin{frame}{Experiment Setup}
	\begin{center}
		\begin{tabular}{|>{\centering\arraybackslash}p{11em}|>{\centering\arraybackslash}p{6em}|>{\centering\arraybackslash}p{6em}|}\hline
			Learning Stage & Continual Learning & Instruction Tuning\\\hline
			LoRA rank & 128 & 64 \\\hline
			epochs    & 1   & 2 \\\hline
			max sequence length (block size) & 2048 & 256 \\\hline
			batch size per GPU & 4 & 144 \\\hline
			gradient accumulation step count & 8 & 4 \\\hline
			scheduler warm-up ratio & 0.05 & 0.03 \\\hline
			optimizer & \multicolumn{2}{c|}{AdamW} \\\hline
			weight decay & \multicolumn{2}{c|}{0.01} \\\hline
			scheduler & \multicolumn{2}{c|}{cosine annealing with warm-up} \\\hline
			Zero Redundancy Optimizer (ZeRO) & \multicolumn{2}{c|}{stage 2, w/o offload} \\\hline
			data type & \multicolumn{2}{c|}{bfloat16} \\\hline
			%peak learning rate & \multicolumn{2}{c|}{0.05} \\\hline
		\end{tabular}
	\end{center}
\end{frame}

\begin{frame}{Sequences Generated w/o Instructions - Distributions}
	\begin{center}
		\includegraphics[scale=0.35]{images/plddt_scperp.pdf}
		\includegraphics[scale=0.35]{images/tm_sid.pdf}
	\end{center}
	\credit{Image}{lv2024prollama}
	\begin{itemize}
		\item 1 spot = 1 sequence
	\end{itemize}
\end{frame}

\end{document}